# End-to-End MLOps Pipeline with MLflow & DagsHub

This project demonstrates the development of a **complete machine learning pipeline**, covering all stages from data ingestion to model prediction with experiment tracking and artifact management.

## Key Components

- **Data Ingestion & Validation**:  
  Automates loading of datasets, schema validation, and quality checks to ensure reliable inputs for training.

- **Data Transformation & Feature Engineering**:  
  Applies preprocessing steps and feature transformations to prepare data for modeling.

- **Model Training & Evaluation**:  
  Trains machine learning models, evaluates performance using metrics like RMSE, MAE, and RÂ², and saves evaluation results.

- **Model Logging & Experiment Tracking**:  
  Uses **MLflow** to log models, metrics, and artifacts, enabling reproducible experiments and centralized tracking.

- **Integration with DagsHub**:  
  Demonstrates remote tracking of MLflow experiments, artifact storage, and model management, ensuring seamless collaboration and version control.

- **Prediction Ready**:  
  Provides a framework for serving models and performing predictions.

## Summary

This project highlights **end-to-end MLOps skills**, including structured pipeline design, experiment tracking, artifact management, and practical use of MLflow and DagsHub for collaborative machine learning workflows.


##Workflow--ML pipeline

1. Data Ingestion
2. Data Validation
3. Data Transformation---data engineering, data preprocessing
4. Model Trainer
5. Model Evaluation

#updation part

1. update config.yaml
2. update schema.yaml
3. update params.yaml
4. update the entity
5.  update the configuration manager in srcc config
6. update the components
7. update the pipeline
8. update the main.py
